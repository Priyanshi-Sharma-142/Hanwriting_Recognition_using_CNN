{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a34e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import nbimporter\n",
    "import tensorflow as tf\n",
    "\n",
    "# Disable eager mode\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "    BestPath = 0\n",
    "    BeamSearch = 1\n",
    "    WordBeamSearch = 2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"minimalistic TF model for HTR\"\n",
    "\n",
    "    # model constants\n",
    "    batchSize = 50\n",
    "    imgSize = (128, 32)\n",
    "    maxTextLen = 32\n",
    "\n",
    "    def __init__(self, charList, decoderType=DecoderType.BestPath, mustRestore=False, dump=False):\n",
    "        \"init model: add CNN, RNN and CTC and initialize TF\"\n",
    "        self.dump = dump\n",
    "        self.charList = charList\n",
    "        self.decoderType = decoderType\n",
    "        self.mustRestore = mustRestore\n",
    "        self.snapID = 0\n",
    "\n",
    "        # Whether to use normalization over a batch or a population\n",
    "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        # input image batch\n",
    "        self.inputImgs = tf.compat.v1.placeholder(tf.float32, shape=(None, Model.imgSize[0], Model.imgSize[1]))\n",
    "\n",
    "        # setup CNN, RNN and CTC\n",
    "        self.setupCNN()\n",
    "        self.setupRNN()\n",
    "        self.setupCTC()\n",
    "\n",
    "        # setup optimizer to train NN\n",
    "        self.batchesTrained = 0\n",
    "        self.learningRate = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
    "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learningRate).minimize(self.loss)\n",
    "\n",
    "        # initialize TF\n",
    "        (self.sess, self.saver) = self.setupTF()\n",
    "\n",
    "    def setupCNN(self):\n",
    "        \"create CNN layers and return output of these layers\"\n",
    "        cnnIn4d = tf.expand_dims(input=self.inputImgs, axis=3)\n",
    "\n",
    "        # list of parameters for the layers\n",
    "        kernelVals = [5, 5, 3, 3, 3]\n",
    "        featureVals = [1, 32, 64, 128, 128, 256]\n",
    "        strideVals = poolVals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
    "        numLayers = len(strideVals)\n",
    "\n",
    "        # create layers\n",
    "        pool = cnnIn4d  # input to first CNN layer\n",
    "        for i in range(numLayers):\n",
    "            kernel = tf.Variable(\n",
    "                tf.random.truncated_normal([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]],\n",
    "                                           stddev=0.1))\n",
    "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
    "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
    "            relu = tf.nn.relu(conv_norm)\n",
    "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, poolVals[i][0], poolVals[i][1], 1),\n",
    "                                    strides=(1, strideVals[i][0], strideVals[i][1], 1), padding='VALID')\n",
    "\n",
    "        self.cnnOut4d = pool\n",
    "\n",
    "    def setupRNN(self):\n",
    "        \"create RNN layers and return output of these layers\"\n",
    "        rnnIn3d = tf.squeeze(self.cnnOut4d, axis=[2])\n",
    "\n",
    "        # basic cells which is used to build RNN\n",
    "        numHidden = 256\n",
    "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=numHidden, state_is_tuple=True) for _ in\n",
    "                 range(2)]  # 2 layers\n",
    "\n",
    "        # stack basic cells\n",
    "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional RNN\n",
    "        # BxTxF -> BxTx2H\n",
    "        ((fw, bw), _) = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d,\n",
    "                                                                  dtype=rnnIn3d.dtype)\n",
    "\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n",
    "        self.rnnOut3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "\n",
    "    def setupCTC(self):\n",
    "        \"create CTC loss and decoder and return them\"\n",
    "        # BxTxC -> TxBxC\n",
    "        self.ctcIn3dTBC = tf.transpose(a=self.rnnOut3d, perm=[1, 0, 2])\n",
    "        # ground truth text as sparse tensor\n",
    "        self.gtTexts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
    "                                       tf.compat.v1.placeholder(tf.int32, [None]),\n",
    "                                       tf.compat.v1.placeholder(tf.int64, [2]))\n",
    "\n",
    "        # calc loss for batch\n",
    "        self.seqLen = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "        self.loss = tf.reduce_mean(input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gtTexts, inputs=self.ctcIn3dTBC,\n",
    "                                                                         sequence_length=self.seqLen,\n",
    "                                                                         ctc_merge_repeated=True))\n",
    "\n",
    "        # calc loss for each element to compute label probability\n",
    "        self.savedCtcInput = tf.compat.v1.placeholder(tf.float32,\n",
    "                                                      shape=[Model.maxTextLen, None, len(self.charList) + 1])\n",
    "        self.lossPerElement = tf.compat.v1.nn.ctc_loss(labels=self.gtTexts, inputs=self.savedCtcInput,\n",
    "                                                       sequence_length=self.seqLen, ctc_merge_repeated=True)\n",
    "\n",
    "        # decoder: either best path decoding or beam search decoding\n",
    "        if self.decoderType == DecoderType.BestPath:\n",
    "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctcIn3dTBC, sequence_length=self.seqLen)\n",
    "        elif self.decoderType == DecoderType.BeamSearch:\n",
    "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctcIn3dTBC, sequence_length=self.seqLen,\n",
    "                                                         beam_width=50)\n",
    "        elif self.decoderType == DecoderType.WordBeamSearch:\n",
    "            # import compiled word beam search operation (see https://github.com/githubharald/CTCWordBeamSearch)\n",
    "            word_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n",
    "\n",
    "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
    "            chars = str().join(self.charList)\n",
    "            wordChars = open('../model/wordCharList.txt').read().splitlines()[0]\n",
    "            corpus = open('../data/corpus.txt').read()\n",
    "\n",
    "            # decode using the \"Words\" mode of word beam search\n",
    "            self.decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(self.ctcIn3dTBC, axis=2), 50, 'Words',\n",
    "                                                                    0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
    "                                                                    wordChars.encode('utf8'))\n",
    "\n",
    "    def setupTF(self):\n",
    "        \"initialize TF\"\n",
    "        print('Python: ' + sys.version)\n",
    "        print('Tensorflow: ' + tf.__version__)\n",
    "\n",
    "        sess = tf.compat.v1.Session()  # TF session\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
    "        modelDir = '../model/'\n",
    "        latestSnapshot = tf.train.latest_checkpoint(modelDir)  # is there a saved model?\n",
    "\n",
    "        # if model must be restored (for inference), there must be a snapshot\n",
    "        if self.mustRestore and not latestSnapshot:\n",
    "            raise Exception('No saved model found in: ' + modelDir)\n",
    "\n",
    "        # load saved model if available\n",
    "        if latestSnapshot:\n",
    "            print('Init with stored values from ' + latestSnapshot)\n",
    "            saver.restore(sess, latestSnapshot)\n",
    "        else:\n",
    "            print('Init with new values')\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        return (sess, saver)\n",
    "\n",
    "    def toSparse(self, texts):\n",
    "        \"put ground truth texts into sparse tensor for ctc_loss\"\n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
    "\n",
    "        # go over all texts\n",
    "        for (batchElement, text) in enumerate(texts):\n",
    "            # convert to string of label (i.e. class-ids)\n",
    "            labelStr = [self.charList.index(c) for c in text]\n",
    "            # sparse tensor must have size of max. label-string\n",
    "            if len(labelStr) > shape[1]:\n",
    "                shape[1] = len(labelStr)\n",
    "            # put each label into sparse tensor\n",
    "            for (i, label) in enumerate(labelStr):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return (indices, values, shape)\n",
    "\n",
    "    def decoderOutputToText(self, ctcOutput, batchSize):\n",
    "        \"extract texts from output of CTC decoder\"\n",
    "\n",
    "        # contains string of labels for each batch element\n",
    "        encodedLabelStrs = [[] for i in range(batchSize)]\n",
    "\n",
    "        # word beam search: label strings terminated by blank\n",
    "        if self.decoderType == DecoderType.WordBeamSearch:\n",
    "            blank = len(self.charList)\n",
    "            for b in range(batchSize):\n",
    "                for label in ctcOutput[b]:\n",
    "                    if label == blank:\n",
    "                        break\n",
    "                    encodedLabelStrs[b].append(label)\n",
    "\n",
    "        # TF decoders: label strings are contained in sparse tensor\n",
    "        else:\n",
    "            # ctc returns tuple, first element is SparseTensor\n",
    "            decoded = ctcOutput[0][0]\n",
    "\n",
    "            # go over all indices and save mapping: batch -> values\n",
    "            idxDict = {b: [] for b in range(batchSize)}\n",
    "            for (idx, idx2d) in enumerate(decoded.indices):\n",
    "                label = decoded.values[idx]\n",
    "                batchElement = idx2d[0]  # index according to [b,t]\n",
    "                encodedLabelStrs[batchElement].append(label)\n",
    "\n",
    "        # map labels to chars for all batch elements\n",
    "        return [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n",
    "\n",
    "    def trainBatch(self, batch):\n",
    "        \"feed a batch into the NN to train it\"\n",
    "        numBatchElements = len(batch.imgs)\n",
    "        sparse = self.toSparse(batch.gtTexts)\n",
    "        rate = 0.01 if self.batchesTrained < 10 else (\n",
    "            0.001 if self.batchesTrained < 10000 else 0.0001)  # decay learning rate\n",
    "        evalList = [self.optimizer, self.loss]\n",
    "        feedDict = {self.inputImgs: batch.imgs, self.gtTexts: sparse,\n",
    "                    self.seqLen: [Model.maxTextLen] * numBatchElements, self.learningRate: rate, self.is_train: True}\n",
    "        (_, lossVal) = self.sess.run(evalList, feedDict)\n",
    "        self.batchesTrained += 1\n",
    "        return lossVal\n",
    "\n",
    "    def dumpNNOutput(self, rnnOutput):\n",
    "        \"dump the output of the NN to CSV file(s)\"\n",
    "        dumpDir = '../dump/'\n",
    "        if not os.path.isdir(dumpDir):\n",
    "            os.mkdir(dumpDir)\n",
    "\n",
    "        # iterate over all batch elements and create a CSV file for each one\n",
    "        maxT, maxB, maxC = rnnOutput.shape\n",
    "        for b in range(maxB):\n",
    "            csv = ''\n",
    "            for t in range(maxT):\n",
    "                for c in range(maxC):\n",
    "                    csv += str(rnnOutput[t, b, c]) + ';'\n",
    "                csv += '\\n'\n",
    "            fn = dumpDir + 'rnnOutput_' + str(b) + '.csv'\n",
    "            print('Write dump of NN to file: ' + fn)\n",
    "            with open(fn, 'w') as f:\n",
    "                f.write(csv)\n",
    "\n",
    "    def inferBatch(self, batch, calcProbability=False, probabilityOfGT=False):\n",
    "        \"feed a batch into the NN to recognize the texts\"\n",
    "\n",
    "        # decode, optionally save RNN output\n",
    "        numBatchElements = len(batch.imgs)\n",
    "        evalRnnOutput = self.dump or calcProbability\n",
    "        evalList = [self.decoder] + ([self.ctcIn3dTBC] if evalRnnOutput else [])\n",
    "        feedDict = {self.inputImgs: batch.imgs, self.seqLen: [Model.maxTextLen] * numBatchElements,\n",
    "                    self.is_train: False}\n",
    "        evalRes = self.sess.run(evalList, feedDict)\n",
    "        decoded = evalRes[0]\n",
    "        texts = self.decoderOutputToText(decoded, numBatchElements)\n",
    "\n",
    "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
    "        probs = None\n",
    "        if calcProbability:\n",
    "            sparse = self.toSparse(batch.gtTexts) if probabilityOfGT else self.toSparse(texts)\n",
    "            ctcInput = evalRes[1]\n",
    "            evalList = self.lossPerElement\n",
    "            feedDict = {self.savedCtcInput: ctcInput, self.gtTexts: sparse,\n",
    "                        self.seqLen: [Model.maxTextLen] * numBatchElements, self.is_train: False}\n",
    "            lossVals = self.sess.run(evalList, feedDict)\n",
    "            probs = np.exp(-lossVals)\n",
    "\n",
    "        # dump the output of the NN to CSV file(s)\n",
    "        if self.dump:\n",
    "            self.dumpNNOutput(evalRes[1])\n",
    "\n",
    "        return (texts, probs)\n",
    "\n",
    "    def save(self):\n",
    "        \"save model to file\"\n",
    "        self.snapID += 1\n",
    "        self.saver.save(self.sess, '../model/snapshot', global_step=self.snapID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a587610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
